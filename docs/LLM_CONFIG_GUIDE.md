# üîê Guia de Configura√ß√£o de LLMs para QAs

Este guia explica, passo a passo, como configurar e utilizar os modelos de linguagem (LLMs) do QA Or√°culo. O p√∫blico-alvo s√£o QAs que desejam preparar o ambiente rapidamente e entender o funcionamento da camada de IA sem precisar mergulhar no c√≥digo.

> **Padr√£o oficial:** O QA Or√°culo j√° vem preparado para usar o **Google Gemini**. Se voc√™ n√£o fizer nenhuma altera√ß√£o, este ser√° o provedor ativo.

---

## ‚úÖ Pr√©-requisitos

- **Python 3.11+** instalado (confira com `python3 --version`).
- Acesso ao reposit√≥rio `qa-oraculo/qa-oraculo-requisitos`.
- Conta no **Google AI Studio** para gerar a chave da API (utilizada pelo Gemini).
- Terminal (macOS/Linux) ou Prompt/PowerShell (Windows) com permiss√µes para executar scripts.

---

## üõ†Ô∏è Passo a passo de configura√ß√£o (Google Gemini ‚Äì padr√£o)

### 1. Clonar o projeto e entrar na pasta
```bash
git clone https://github.com/seu-usuario/qa-oraculo.git
cd qa-oraculo/qa-oraculo-requisitos
```

### 2. Criar e ativar o ambiente virtual
```bash
python3 -m venv .venv
source .venv/bin/activate     # macOS/Linux
# .venv\Scripts\activate     # Windows
```

### 3. Instalar depend√™ncias
```bash
pip install -r requirements.txt
```

### 4. Criar o arquivo `.env`
```bash
cat <<'EOF' > .env
LLM_PROVIDER="google"
LLM_MODEL="gemini-2.0-flash-lite-001"
GOOGLE_API_KEY="SUA_CHAVE_DO_GEMINI"
EOF
```

#### Onde encontrar a Google API Key
1. Acesse [https://aistudio.google.com](https://aistudio.google.com).
2. Clique em **Dashboard ‚Üí API Keys ‚Üí Create API Key**.
3. Copie a chave e cole no `.env` no campo `GOOGLE_API_KEY`.
4. Guarde essa chave em um cofre de segredos (Vault, 1Password, etc.).

### 5. Executar o QA Or√°culo
```bash
streamlit run main.py
```

O navegador abrir√° automaticamente em `http://localhost:8501`. Se isso n√£o ocorrer, cole o endere√ßo manualmente.

---

## üåê Vari√°veis por provedor

A arquitetura permite escolher o provedor via `LLM_PROVIDER`. A tabela abaixo lista as vari√°veis esperadas e o status atual do suporte.

| Provedor (`LLM_PROVIDER`) | Status | Vari√°veis necess√°rias | Onde pegar essas informa√ß√µes |
|---------------------------|--------|------------------------|------------------------------|
| `google` (padr√£o) | ‚úÖ Ativo | `GOOGLE_API_KEY` <br> `LLM_MODEL` (opcional, default Gemini) | **Google AI Studio** (Dashboard ‚Üí API Keys) |
| `azure` / `azure_openai` | ‚ö†Ô∏è Em prepara√ß√£o | `AZURE_OPENAI_API_KEY` <br> `AZURE_OPENAI_ENDPOINT` <br> `AZURE_OPENAI_DEPLOYMENT` <br> `AZURE_OPENAI_API_VERSION` | **Portal Azure** ‚Üí Azure OpenAI ‚Üí `Keys & Endpoint` / `Deployments` |
| `openai` / `gpt` | ‚ö†Ô∏è Em prepara√ß√£o | `OPENAI_API_KEY` <br> `OPENAI_BASE_URL` (opcional) <br> `OPENAI_ORGANIZATION` (opcional) | **OpenAI Platform** ‚Üí User menu ‚Üí `View API keys` / Organization settings |

> Mesmo para provedores ainda n√£o suportados, configurar o `.env` com anteced√™ncia ajuda a identificar o que falta quando o suporte for liberado.

---

## üîÑ Como alternar entre provedores

### Mantendo o padr√£o (Google Gemini)
- Verifique se `LLM_PROVIDER="google"` est√° no `.env`.
- Certifique-se de que `GOOGLE_API_KEY` est√° preenchida.
- Opcionalmente ajuste `LLM_MODEL` se quiser usar outro modelo Gemini compat√≠vel.

### Azure OpenAI (estrutura pronta, integra√ß√£o em desenvolvimento)
```bash
LLM_PROVIDER="azure"
LLM_MODEL="gpt-4o"
AZURE_OPENAI_API_KEY="chave_azure"
AZURE_OPENAI_ENDPOINT="https://sua-instancia.openai.azure.com"
AZURE_OPENAI_DEPLOYMENT="nome-do-deployment"
AZURE_OPENAI_API_VERSION="2024-02-15-preview"
```
- **Onde obter:** Portal Azure ‚Üí Recurso Azure OpenAI ‚Üí menu `Keys & Endpoint` (pegar endpoint e chave) e `Deployments` (nome do deployment e vers√£o da API).
- **Status atual:** o QA Or√°culo valida se todos os campos foram preenchidos e informa claramente quais vari√°veis ainda faltam. A chamada ao modelo ainda n√£o est√° habilitada.

### OpenAI GPT (estrutura pronta, integra√ß√£o em desenvolvimento)
```bash
LLM_PROVIDER="openai"
LLM_MODEL="gpt-4.1"
OPENAI_API_KEY="chave_do_openai"
# OPENAI_BASE_URL="https://api.openai.com/v1"      # opcional
# OPENAI_ORGANIZATION="org_xxxxx"                 # opcional
```
- **Onde obter:** OpenAI Platform ‚Üí User (canto superior direito) ‚Üí `View API keys`. Organization ID em `Settings ‚Üí Organizations`.
- **Status atual:** semelhante ao Azure, o QA Or√°culo valida vari√°veis e informa que a integra√ß√£o ser√° liberada em uma vers√£o futura.

### Alternando rapidamente
1. Edite o `.env` com o provedor desejado.
2. Salve e **reinicie** o Streamlit (`Ctrl+C` para parar, depois `streamlit run main.py`).
3. Se quiser voltar para o padr√£o, restaure `LLM_PROVIDER="google"` e garanta que `GOOGLE_API_KEY` esteja presente.

---

## üß† Como a camada de LLM funciona

1. **Leitura do `.env`**: o projeto carrega `LLM_PROVIDER`, `LLM_MODEL` e chaves espec√≠ficas conforme a op√ß√£o escolhida.
2. **F√°brica de provedores**: o QA Or√°culo seleciona automaticamente o driver. Google j√° est√° implementado; Azure/OpenAI retornam mensagens indicando o que falta.
3. **Chamadas com retry e observabilidade**: toda chamada registra eventos (`model.call.start`, `model.call.success`, `model.call.error`) com **trace IDs** para troubleshooting.
4. **Resultados**: a IA retorna JSONs estruturados com an√°lise, plano de testes e relat√≥rios. Falhas geram mensagens amig√°veis e logs detalhados.

---

## üë©‚Äçüíª Fluxo t√≠pico para QAs

1. **Cole a User Story** na √°rea indicada.
2. Clique em **"Gerar an√°lise"**.
3. Aguarde o processamento: mensagens de status e logs aparecem na lateral.
4. Revise a an√°lise inicial, perguntas ao PO, crit√©rios de aceite e riscos.
5. Clique em **"Gerar plano de testes"** para receber cen√°rios Gherkin, resumo e prioriza√ß√£o.
6. Utilize os bot√µes de **exporta√ß√£o** (Markdown, PDF, Xray, etc.).
7. Consulte o hist√≥rico para revisitar an√°lises anteriores.

> Se algo aparentar travar, abra o terminal onde o Streamlit est√° rodando: os logs do LangGraph mostram o estado de cada n√≥ e os tempos de execu√ß√£o (√∫til para QA investigar gargalos ou problemas de quota).

---

## üßØ Troubleshooting r√°pido

| Sintoma | Poss√≠vel causa | Solu√ß√£o sugerida |
|---------|----------------|------------------|
| `LLMError: GOOGLE_API_KEY n√£o configurada` | `.env` incompleto ou vari√°vel mal escrita | Verifique se `GOOGLE_API_KEY` consta no `.env` e se o arquivo est√° na raiz do projeto |
| `LLMError: Azure OpenAI requer vari√°veis...` | Vari√°vel obrigat√≥ria do Azure ausente | Preencha todas as vari√°veis listadas na tabela de provedores |
| `LLMError: OpenAI GPT ainda n√£o suportado` | Integra√ß√£o em desenvolvimento | Aguarde a vers√£o correspondente ou acompanhe o roadmap |
| `LLMRateLimitError` | Limite de requisi√ß√µes do Gemini atingido | Aguarde alguns minutos e tente novamente. Para evitar reincid√™ncia, alinhe quotas com o time |
| Resposta vazia / relat√≥rio em fallback | Instabilidade tempor√°ria do provedor | Tente novamente e consulte os logs (`model.call.error`) |
| `streamlit run` n√£o abre navegador | Porta ocupada ou Streamlit em segundo plano | Use `streamlit run main.py --server.port 8502` ou encerre inst√¢ncias anteriores |

---

## üìå Boas pr√°ticas para equipes de QA

- **Centralize as chaves** em um cofre seguro e distribua com parcim√¥nia.
- **Defina quotas internas** para evitar estouro dos limites da API durante sprints.
- **Revise sempre** as an√°lises geradas pela IA antes de exportar.
- **Registre feedbacks**: logs estruturados ajudam a reportar problemas com embasamento.

---

## üÜò Precisa de ajuda?

- Visite o [README](../README.md) para vis√£o geral do projeto.
- Confira o [CHANGELOG](CHANGELOG.md) para novidades.
- Abra uma issue no GitHub (Issues) ou contate o time respons√°vel com o trace ID e prints do erro.

**Bons testes com o QA Or√°culo! üí°**

# üîê Guia de Configura√ß√£o de LLMs para QAs

Este guia explica, passo a passo, como configurar e utilizar os modelos de linguagem (LLMs) do QA Or√°culo. O p√∫blico-alvo s√£o QAs que desejam preparar o ambiente rapidamente e entender o funcionamento da camada de IA sem precisar mergulhar no c√≥digo.

> **Padr√£o oficial:** O QA Or√°culo j√° vem preparado para usar o **Google Gemini**. Se voc√™ n√£o fizer nenhuma altera√ß√£o, este ser√° o provedor ativo.

---

## ‚úÖ Pr√©-requisitos

- **Python 3.11+** instalado (confira com `python3 --version`).
- Acesso ao reposit√≥rio `qa-oraculo/qa-oraculo-requisitos`.
- Conta no **Google AI Studio** para gerar a chave da API (utilizada pelo Gemini).
- Terminal (macOS/Linux) ou Prompt/PowerShell (Windows) com permiss√µes para executar scripts.

---

## üõ†Ô∏è Passo a passo de configura√ß√£o (Google Gemini ‚Äì padr√£o)

### 1. Clonar o projeto e entrar na pasta
```bash
git clone https://github.com/seu-usuario/qa-oraculo.git
cd qa-oraculo/qa-oraculo-requisitos
```

### 2. Criar e ativar o ambiente virtual
```bash
python3 -m venv .venv
source .venv/bin/activate     # macOS/Linux
# .venv\Scripts\activate     # Windows
```

### 3. Instalar depend√™ncias
```bash
pip install -r requirements.txt
```

### 4. Criar o arquivo `.env`
```bash
cat <<'EOF' > .env
LLM_PROVIDER="google"
LLM_MODEL="gemini-2.0-flash-lite-001"
GOOGLE_API_KEY="SUA_CHAVE_DO_GEMINI"
EOF
```

#### Onde encontrar a Google API Key
1. Acesse [https://aistudio.google.com](https://aistudio.google.com).
2. Clique em **Dashboard ‚Üí API Keys ‚Üí Create API Key**.
3. Copie a chave e cole no `.env` no campo `GOOGLE_API_KEY`.
4. Guarde essa chave em um cofre de segredos (Vault, 1Password, etc.).

### 5. Executar o QA Or√°culo
```bash
streamlit run main.py
```

O navegador abrir√° automaticamente em `http://localhost:8501`. Se isso n√£o ocorrer, cole o endere√ßo manualmente.

---

## üåê Vari√°veis por provedor

A arquitetura permite escolher o provedor via `LLM_PROVIDER`. A tabela abaixo lista as vari√°veis esperadas e o status atual do suporte.

| Provedor (`LLM_PROVIDER`) | Status | Vari√°veis necess√°rias | Onde pegar essas informa√ß√µes |
|---------------------------|--------|------------------------|------------------------------|
| `google` (padr√£o) | ‚úÖ Ativo | `GOOGLE_API_KEY` <br> `LLM_MODEL` (opcional) | **Google AI Studio** ‚Üí [Documenta√ß√£o](https://ai.google.dev/gemini-api/docs) |
| `azure` / `azure_openai` | ‚úÖ Ativo | `AZURE_OPENAI_API_KEY` <br> `AZURE_OPENAI_ENDPOINT` <br> `AZURE_OPENAI_DEPLOYMENT` <br> `AZURE_OPENAI_API_VERSION` | **Portal Azure** ‚Üí [Documenta√ß√£o oficial](https://learn.microsoft.com/azure/ai-services/openai/) |
| `openai` / `gpt` | ‚úÖ Ativo | `OPENAI_API_KEY` <br> `OPENAI_ORGANIZATION` (opcional) | **OpenAI Platform** ‚Üí [Documenta√ß√£o oficial](https://platform.openai.com/docs) |
| `llama` | ‚úÖ Ativo (Ollama) | **Nenhuma** (local e gratuito!) | **Ollama** ‚Üí [https://ollama.ai](https://ollama.ai) |

---

## üîÑ Como alternar entre provedores

### Mantendo o padr√£o (Google Gemini)
- Verifique se `LLM_PROVIDER="google"` est√° no `.env`.
- Certifique-se de que `GOOGLE_API_KEY` est√° preenchida.
- Opcionalmente ajuste `LLM_MODEL` se quiser usar outro modelo Gemini compat√≠vel.

### Azure OpenAI ‚úÖ **DISPON√çVEL**
```bash
LLM_PROVIDER="azure"
LLM_MODEL="gpt-4"
AZURE_OPENAI_API_KEY="sua-chave-azure"
AZURE_OPENAI_ENDPOINT="https://sua-instancia.openai.azure.com"
AZURE_OPENAI_DEPLOYMENT="nome-do-deployment"
AZURE_OPENAI_API_VERSION="2024-02-15-preview"
```

**Onde obter:**
1. Acesse o [Portal Azure](https://portal.azure.com)
2. Navegue at√© seu recurso Azure OpenAI
3. Menu `Keys & Endpoint`: copie a chave e o endpoint
4. Menu `Deployments`: copie o nome do deployment
5. Use a vers√£o da API recomendada (ex: `2024-02-15-preview`)

**Status:** ‚úÖ Totalmente funcional! Suporta GPT-4, GPT-3.5-turbo e outros modelos dispon√≠veis no Azure.

### OpenAI GPT ‚úÖ **DISPON√çVEL**
```bash
LLM_PROVIDER="openai"
LLM_MODEL="gpt-4"
OPENAI_API_KEY="sk-..."
# OPENAI_ORGANIZATION="org-xxxxx"  # opcional
```

**Onde obter:**
1. Acesse [OpenAI Platform](https://platform.openai.com)
2. Clique em seu perfil (canto superior direito) ‚Üí `View API keys`
3. Crie uma nova chave ou use uma existente
4. (Opcional) Em `Settings ‚Üí Organizations`, copie o Organization ID

**Status:** ‚úÖ Totalmente funcional! Suporta GPT-4, GPT-3.5-turbo, GPT-4-turbo e outros modelos.

### LLaMA (Ollama) ‚úÖ **DISPON√çVEL E GRATUITO** üéâ
```bash
LLM_PROVIDER="llama"
LLM_MODEL="llama2"
# N√£o precisa de API key!
```

**Como configurar:**

1. **Instalar Ollama:**
   - Acesse [https://ollama.ai](https://ollama.ai)
   - Baixe e instale para seu sistema operacional
   - Verifique a instala√ß√£o: `ollama --version`

2. **Baixar um modelo:**
   ```bash
   ollama pull llama2        # LLaMA 2 (7B)
   ollama pull llama2:13b    # LLaMA 2 (13B)
   ollama pull llama3        # LLaMA 3 (mais recente)
   ```

3. **Verificar que Ollama est√° rodando:**
   ```bash
   ollama list  # Lista modelos instalados
   ```

4. **Configurar o `.env`:**
   ```bash
   LLM_PROVIDER="llama"
   LLM_MODEL="llama2"  # ou llama3, llama2:13b, etc.
   ```

**Vantagens:**
- ‚úÖ **100% Gratuito** - roda localmente
- ‚úÖ **Privacidade total** - dados n√£o saem da sua m√°quina
- ‚úÖ **Sem limites de quota** - use quanto quiser
- ‚úÖ **Offline** - funciona sem internet (ap√≥s baixar o modelo)

**Modelos dispon√≠veis:**
- `llama2` - LLaMA 2 (7B) - R√°pido e leve
- `llama2:13b` - LLaMA 2 (13B) - Mais preciso
- `llama3` - LLaMA 3 - Vers√£o mais recente
- E muitos outros! Veja a lista completa em [https://ollama.ai/library](https://ollama.ai/library)

**Status:** ‚úÖ Totalmente funcional! Perfeito para desenvolvimento local e testes.

### Alternando rapidamente
1. Edite o `.env` com o provedor desejado.
2. Salve e **reinicie** o Streamlit (`Ctrl+C` para parar, depois `streamlit run main.py`).
3. Se quiser voltar para o padr√£o, restaure `LLM_PROVIDER="google"` e garanta que `GOOGLE_API_KEY` esteja presente.

---

## üß† Como a camada de LLM funciona

1. **Leitura do `.env`**: o projeto carrega `LLM_PROVIDER`, `LLM_MODEL` e chaves espec√≠ficas conforme a op√ß√£o escolhida.
2. **F√°brica de provedores**: o QA Or√°culo seleciona automaticamente o driver correto (Google, Azure, OpenAI ou Ollama).
3. **Chamadas com retry e observabilidade**: toda chamada registra eventos (`model.call.start`, `model.call.success`, `model.call.error`) com **trace IDs** para troubleshooting.
4. **Resultados**: a IA retorna JSONs estruturados com an√°lise, plano de testes e relat√≥rios. Falhas geram mensagens amig√°veis e logs detalhados.

---

## üë©‚Äçüíª Fluxo t√≠pico para QAs

1. **Cole a User Story** na √°rea indicada.
2. Clique em **"Gerar an√°lise"**.
3. Aguarde o processamento: mensagens de status e logs aparecem na lateral.
4. Revise a an√°lise inicial, perguntas ao PO, crit√©rios de aceite e riscos.
5. Clique em **"Gerar plano de testes"** para receber cen√°rios Gherkin, resumo e prioriza√ß√£o.
6. Utilize os bot√µes de **exporta√ß√£o** (Markdown, PDF, Xray, etc.).
7. Consulte o hist√≥rico para revisitar an√°lises anteriores.

> Se algo aparentar travar, abra o terminal onde o Streamlit est√° rodando: os logs do LangGraph mostram o estado de cada n√≥ e os tempos de execu√ß√£o (√∫til para QA investigar gargalos ou problemas de quota).

---

## üßØ Troubleshooting r√°pido

| Sintoma | Poss√≠vel causa | Solu√ß√£o sugerida |
|---------|----------------|------------------|
| `LLMError: GOOGLE_API_KEY n√£o configurada` | `.env` incompleto ou vari√°vel mal escrita | Verifique se `GOOGLE_API_KEY` consta no `.env` e se o arquivo est√° na raiz do projeto |
| `LLMError: Azure OpenAI requer vari√°veis...` | Vari√°vel obrigat√≥ria do Azure ausente | Preencha todas as vari√°veis listadas na tabela de provedores |
| `LLMError: OPENAI_API_KEY n√£o configurada` | API key do OpenAI ausente | Adicione `OPENAI_API_KEY` ao `.env` |
| `LLMError: Ollama n√£o est√° acess√≠vel` | Ollama n√£o est√° instalado ou n√£o est√° rodando | Instale Ollama ([https://ollama.ai](https://ollama.ai)) e verifique com `ollama list` |
| `LLMError: modelo 'llama3' not found` | Modelo n√£o foi baixado | Execute `ollama pull llama3` para baixar o modelo |
| `LLMRateLimitError` | Limite de requisi√ß√µes atingido | Aguarde alguns minutos e tente novamente. Para evitar reincid√™ncia, alinhe quotas com o time |
| Resposta vazia / relat√≥rio em fallback | Instabilidade tempor√°ria do provedor | Tente novamente e consulte os logs (`model.call.error`) |
| `streamlit run` n√£o abre navegador | Porta ocupada ou Streamlit em segundo plano | Use `streamlit run main.py --server.port 8502` ou encerre inst√¢ncias anteriores |

---

## üìå Boas pr√°ticas para equipes de QA

- **Centralize as chaves** em um cofre seguro e distribua com parcim√¥nia.
- **Defina quotas internas** para evitar estouro dos limites da API durante sprints.
- **Use Ollama para desenvolvimento local** - economize custos e mantenha privacidade.
- **Revise sempre** as an√°lises geradas pela IA antes de exportar.
- **Registre feedbacks**: logs estruturados ajudam a reportar problemas com embasamento.

---

## üÜò Precisa de ajuda?

- Visite o [README](../README.md) para vis√£o geral do projeto.
- Confira o [CHANGELOG](CHANGELOG.md) para novidades.
- Abra uma issue no GitHub (Issues) ou contate o time respons√°vel com o trace ID e prints do erro.

**Bons testes com o QA Or√°culo! üí°**

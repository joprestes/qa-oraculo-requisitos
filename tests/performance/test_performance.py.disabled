"""
Testes de performance para operações críticas do QA Oráculo.

Usa pytest-benchmark para medir e comparar performance de operações importantes.
"""

import pytest
from unittest.mock import patch
from qa_core.database import save_analysis_to_history, get_all_analysis_history
from qa_core.llm.factory import get_llm_client
from qa_core.utils.exporters import (
    export_to_markdown,
    export_to_pdf,
    export_to_azure_csv,
)


@pytest.fixture
def sample_analysis():
    """Fixture com análise de exemplo para benchmarks."""
    return {
        "user_story": "Como usuário, quero fazer login no sistema",
        "analysis_report": {
            "criterios_aceitacao": [
                "Sistema deve validar credenciais",
                "Sistema deve redirecionar após login",
            ],
            "regras_negocio": ["Senha deve ter mínimo 8 caracteres"],
            "cenarios_teste": [
                {
                    "titulo": "Login com sucesso",
                    "passos": ["Acessar tela de login", "Informar credenciais válidas"],
                    "resultado_esperado": "Usuário autenticado",
                }
            ],
        },
        "test_plan": {
            "scenarios": [
                {
                    "title": "Login com sucesso",
                    "steps": ["Dado que estou na tela de login"],
                    "expected": "Então devo ser autenticado",
                }
            ]
        },
    }


class TestDatabasePerformance:
    """Testes de performance para operações de banco de dados."""

    def test_save_analysis_performance(self, benchmark, sample_analysis):
        """Benchmark para salvar análise no banco de dados."""

        def save_analysis():
            save_analysis_to_history(
                user_story=sample_analysis["user_story"],
                analysis_report=sample_analysis["analysis_report"],
                test_plan=sample_analysis["test_plan"],
            )

        result = benchmark(save_analysis)
        assert result is None

    def test_get_history_performance(self, benchmark, sample_analysis):
        """Benchmark para recuperar histórico do banco de dados."""

        # Inserir 10 análises para teste
        for i in range(10):
            save_analysis_to_history(
                user_story=f"User Story {i}",
                analysis_report=sample_analysis["analysis_report"],
                test_plan=sample_analysis["test_plan"],
            )

        def get_history():
            return get_all_analysis_history()

        result = benchmark(get_history)
        assert len(result) >= 10  # Pode ter mais de 10 se já existiam análises


class TestExportPerformance:
    """Testes de performance para exportações."""

    def test_markdown_export_performance(self, benchmark, sample_analysis):
        """Benchmark para exportação Markdown."""

        def export_markdown():
            return export_to_markdown(
                user_story=sample_analysis["user_story"],
                analysis_report=sample_analysis["analysis_report"],
                test_plan=sample_analysis["test_plan"],
            )

        result = benchmark(export_markdown)
        assert result is not None
        assert len(result) > 0

    def test_pdf_export_performance(self, benchmark, sample_analysis):
        """Benchmark para exportação PDF."""

        def export_pdf():
            return export_to_pdf(
                user_story=sample_analysis["user_story"],
                analysis_report=sample_analysis["analysis_report"],
                test_plan=sample_analysis["test_plan"],
            )

        result = benchmark(export_pdf)
        assert result is not None
        assert len(result) > 0

    def test_csv_export_performance(self, benchmark, sample_analysis):
        """Benchmark para exportação CSV (Azure DevOps)."""

        def export_csv():
            return export_to_azure_csv(
                test_plan=sample_analysis["test_plan"],
                user_story=sample_analysis["user_story"],
            )

        result = benchmark(export_csv)
        assert result is not None
        assert len(result) > 0


class TestLLMPerformance:
    """Testes de performance para operações com LLM."""

    @patch("qa_core.llm.providers.mock.MockLLMClient.generate_content")
    def test_llm_generate_performance(self, mock_generate, benchmark):
        """Benchmark para geração de conteúdo com LLM (mockado)."""
        mock_generate.return_value = "Análise de teste gerada"

        client = get_llm_client(provider="mock")

        def generate_content():
            return client.generate_content(
                prompt="Analise esta user story: Como usuário, quero fazer login"
            )

        result = benchmark(generate_content)
        assert result is not None


class TestCachePerformance:
    """Testes de performance para operações de cache."""

    @patch("qa_core.llm.providers.mock.MockLLMClient.generate_content")
    def test_cache_hit_performance(self, mock_generate, benchmark):
        """Benchmark para cache hit (mesma chamada repetida)."""
        mock_generate.return_value = "Resposta mockada"

        client = create_llm_client(provider="mock", use_cache=True)
        prompt = "Teste de cache"

        # Primeira chamada para popular cache
        client.generate_content(prompt=prompt)

        # Benchmark da segunda chamada (deve usar cache)
        def cached_call():
            return client.generate_content(prompt=prompt)

        result = benchmark(cached_call)
        assert result is not None

    @patch("qa_core.llm.providers.mock.MockLLMClient.generate_content")
    def test_cache_miss_performance(self, mock_generate, benchmark):
        """Benchmark para cache miss (chamadas diferentes)."""
        mock_generate.return_value = "Resposta mockada"

        client = create_llm_client(provider="mock", use_cache=True)

        call_count = [0]

        def uncached_call():
            call_count[0] += 1
            return client.generate_content(prompt=f"Prompt único {call_count[0]}")

        result = benchmark(uncached_call)
        assert result is not None
